{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcb4020",
   "metadata": {},
   "source": [
    "🔄 Train-Test-Validation (OOT) Split Strategy\n",
    "Since this is time-series data:\n",
    "\n",
    "Train: earliest 70% of the timeline (e.g., Jan 2019–Jul 2020).\n",
    "\n",
    "Test: next 15% (e.g., Aug–Oct 2020).\n",
    "\n",
    "OOT (Out-of-Time validation): final 15% (e.g., Nov–Dec 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159ffc4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a9639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Aggregate shape: (538, 2)\n",
      "Weekly Aggregate shape: (77, 2)\n",
      "Monthly Aggregate shape: (18, 2)\n",
      "Transaction Features shape: (1296675, 55)\n",
      "Per-card Summary shape: (983, 42)\n",
      "Per-card Daily Panel shape: (488031, 3)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Load DataFrames from exported CSVs\n",
    "daily_aggregate = pd.read_csv('data_exports/daily_aggregate.csv', parse_dates=['trans_dt'])\n",
    "weekly_aggregate = pd.read_csv('data_exports/weekly_aggregate.csv', parse_dates=['trans_dt'])\n",
    "monthly_aggregate = pd.read_csv('data_exports/monthly_aggregate.csv', parse_dates=['trans_dt'])\n",
    "\n",
    "\n",
    "transaction_features = pd.read_csv('data_exports/transaction_features.csv', parse_dates=['trans_dt'])\n",
    "per_card_summary = pd.read_csv('data_exports/per_card_summary.csv')\n",
    "per_card_daily_panel = pd.read_csv('data_exports/per_card_daily_panel.csv', parse_dates=['trans_dt'])\n",
    "\n",
    "# Sanity checks\n",
    "print(\"Daily Aggregate shape:\", daily_aggregate.shape)\n",
    "print(\"Weekly Aggregate shape:\", weekly_aggregate.shape)\n",
    "print(\"Monthly Aggregate shape:\", monthly_aggregate.shape)\n",
    "print(\"Transaction Features shape:\", transaction_features.shape)\n",
    "print(\"Per-card Summary shape:\", per_card_summary.shape)\n",
    "print(\"Per-card Daily Panel shape:\", per_card_daily_panel.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d99c4d",
   "metadata": {},
   "source": [
    "### Loading in and Checking the Transaction Level + Per Card Time Series Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96453829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== transaction_features columns ===\n",
      "['cc_num', 'merchant', 'category', 'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud', 'amt_outlier', 'trans_dt', 'hour', 'weekday', 'month', 'spend_entertainment', 'spend_food_dining', 'spend_gas_transport', 'spend_grocery_net', 'spend_grocery_pos', 'spend_health_fitness', 'spend_home', 'spend_kids_pets', 'spend_misc_net', 'spend_misc_pos', 'spend_personal_care', 'spend_shopping_net', 'spend_shopping_pos', 'spend_travel', 'frac_spend_entertainment', 'frac_spend_food_dining', 'frac_spend_gas_transport', 'frac_spend_grocery_net', 'frac_spend_grocery_pos', 'frac_spend_health_fitness', 'frac_spend_home', 'frac_spend_kids_pets', 'frac_spend_misc_net', 'frac_spend_misc_pos', 'frac_spend_personal_care', 'frac_spend_shopping_net', 'frac_spend_shopping_pos', 'frac_spend_travel', 'is_top300_merchant']\n",
      "\n",
      "=== per_card_summary columns ===\n",
      "['txn_count', 'total_spend', 'mean_spend', 'std_spend', 'first_txn', 'last_txn', 'fraud_rate', 'days_active', 'spend_per_day', 'spend_entertainment', 'spend_food_dining', 'spend_gas_transport', 'spend_grocery_net', 'spend_grocery_pos', 'spend_health_fitness', 'spend_home', 'spend_kids_pets', 'spend_misc_net', 'spend_misc_pos', 'spend_personal_care', 'spend_shopping_net', 'spend_shopping_pos', 'spend_travel', 'frac_spend_entertainment', 'frac_spend_food_dining', 'frac_spend_gas_transport', 'frac_spend_grocery_net', 'frac_spend_grocery_pos', 'frac_spend_health_fitness', 'frac_spend_home', 'frac_spend_kids_pets', 'frac_spend_misc_net', 'frac_spend_misc_pos', 'frac_spend_personal_care', 'frac_spend_shopping_net', 'frac_spend_shopping_pos', 'frac_spend_travel', 'home_city', 'home_state', 'city_pop', 'city_total_spend', 'state_total_spend']\n",
      "\n",
      "=== per_card_daily_panel columns ===\n",
      "['cc_num', 'trans_dt', 'amt']\n"
     ]
    }
   ],
   "source": [
    "# load your exports\n",
    "\n",
    "\n",
    "# print the column lists\n",
    "print(\"=== transaction_features columns ===\")\n",
    "print(transaction_features.columns.tolist(), end=\"\\n\\n\")\n",
    "\n",
    "print(\"=== per_card_summary columns ===\")\n",
    "print(per_card_summary.columns.tolist(), end=\"\\n\\n\")\n",
    "\n",
    "print(\"=== per_card_daily_panel columns ===\")\n",
    "print(per_card_daily_panel.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b5aab2",
   "metadata": {},
   "source": [
    "## SPLITTING INTO COHORTS TRAIN, TEST, AND OOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be7bc1",
   "metadata": {},
   "source": [
    "### Per_Card_Daily_Panel_Columns Splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccefa819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01 00:00:00 2020-06-21 00:00:00\n",
      "Train period: 2019-01-01 00:00:00 to 2020-01-31 00:00:00\n",
      "Test period: 2020-02-01 00:00:00 to 2020-04-15 00:00:00\n",
      "OOT period: 2020-04-16 00:00:00 to 2020-06-21 00:00:00\n",
      "Train size: 359402\n",
      "Test size: 68114\n",
      "OOT size: 60515\n"
     ]
    }
   ],
   "source": [
    "panel = per_card_daily_panel.copy()\n",
    "\n",
    "# First ensure dates are sorted\n",
    "panel = panel.sort_values(by=['cc_num', 'trans_dt'])\n",
    "\n",
    "# Check range\n",
    "print(panel['trans_dt'].min(), panel['trans_dt'].max())\n",
    "\n",
    "# Define split dates\n",
    "train_end = '2020-01-31'\n",
    "test_end = '2020-04-15'\n",
    "\n",
    "# Create splits\n",
    "daily_train_panel = panel[panel['trans_dt'] <= train_end]\n",
    "daily_test_panel = panel[(panel['trans_dt'] > train_end) & (panel['trans_dt'] <= test_end)]\n",
    "daily_oot_panel = panel[panel['trans_dt'] > test_end]\n",
    "\n",
    "# Sanity checks\n",
    "print(\"Train period:\", daily_train_panel['trans_dt'].min(), \"to\", daily_train_panel['trans_dt'].max())\n",
    "print(\"Test period:\", daily_test_panel['trans_dt'].min(), \"to\", daily_test_panel['trans_dt'].max())\n",
    "print(\"OOT period:\", daily_oot_panel['trans_dt'].min(), \"to\", daily_oot_panel['trans_dt'].max())\n",
    "\n",
    "print(\"Train size:\", len(daily_train_panel))\n",
    "print(\"Test size:\", len(daily_test_panel))\n",
    "print(\"OOT size:\", len(daily_oot_panel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3192d8",
   "metadata": {},
   "source": [
    "## Transaction Feature Store Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af2487a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction Features Splits:\n",
      "Train: 2019-01-01 00:00:18 to 2020-01-30 23:58:55\n",
      "Test: 2020-01-31 00:01:33 to 2020-04-14 23:59:47\n",
      "OOT: 2020-04-15 00:00:01 to 2020-06-21 12:13:37\n",
      "Sizes:\n",
      "Train size: 975662\n",
      "Test size: 154141\n",
      "OOT size: 166872\n"
     ]
    }
   ],
   "source": [
    "transaction_features = transaction_features.sort_values('trans_dt')\n",
    "\n",
    "train_end = '2020-01-31'\n",
    "test_end = '2020-04-15'\n",
    "\n",
    "train_transactions = transaction_features[transaction_features['trans_dt'] <= train_end]\n",
    "test_transactions = transaction_features[\n",
    "    (transaction_features['trans_dt'] > train_end) & \n",
    "    (transaction_features['trans_dt'] <= test_end)\n",
    "]\n",
    "oot_transactions = transaction_features[transaction_features['trans_dt'] > test_end]\n",
    "\n",
    "print(\"Transaction Features Splits:\")\n",
    "print(\"Train:\", train_transactions['trans_dt'].min(), \"to\", train_transactions['trans_dt'].max())\n",
    "print(\"Test:\", test_transactions['trans_dt'].min(), \"to\", test_transactions['trans_dt'].max())\n",
    "print(\"OOT:\", oot_transactions['trans_dt'].min(), \"to\", oot_transactions['trans_dt'].max())\n",
    "\n",
    "print(\"Sizes:\")\n",
    "print(\"Train size:\", len(train_transactions))\n",
    "print(\"Test size:\", len(test_transactions))\n",
    "print(\"OOT size:\", len(oot_transactions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9584f",
   "metadata": {},
   "source": [
    "## Per Card Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a4f3d",
   "metadata": {},
   "source": [
    "Strategy:\n",
    "Split based on a combination of first_txn and ensuring that cards in test and OOT have sufficient activity. For instance, you could split by cards that have activity through the full dataset, but then split transactions by date per card, rather than splitting by card cohorts directly.\n",
    "\n",
    "Recommended rigorous approach (Time-Series Per-Card Splitting):\n",
    "Instead of splitting the per-card summary directly, it’s often better to:\n",
    "\n",
    "Use the full set of cards in all splits.\n",
    "\n",
    "Split each card’s transactions by date into train/test/OOT periods.\n",
    "\n",
    "Thus, each card appears in all cohorts, but the dates for the transactions are strictly separated. This is standard in time series forecasting to predict future behavior from past behavior per card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21180e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction-based splits:\n",
      "Train period: 2019-01-01 00:00:00 to 2020-01-31 00:00:00\n",
      "Test period: 2020-02-01 00:00:00 to 2020-04-15 00:00:00\n",
      "OOT period: 2020-04-16 00:00:00 to 2020-06-21 00:00:00\n",
      "\n",
      "Number of unique cards per period:\n",
      "Train cards: 963\n",
      "Test cards: 915\n",
      "OOT cards: 921\n",
      "\n",
      "Sizes (number of rows):\n",
      "Train rows: 359402\n",
      "Test rows: 68114\n",
      "OOT rows: 60515\n"
     ]
    }
   ],
   "source": [
    "per_card_summary['first_txn'] = pd.to_datetime(per_card_summary['first_txn'])\n",
    "\n",
    "# Cohort assignment based on first transaction date\n",
    "train_end = '2020-01-31'\n",
    "test_end = '2020-04-15'\n",
    "\n",
    "# Per-card transaction splits:\n",
    "train_panel = per_card_daily_panel[per_card_daily_panel['trans_dt'] <= train_end]\n",
    "test_panel = per_card_daily_panel[\n",
    "    (per_card_daily_panel['trans_dt'] > train_end) & \n",
    "    (per_card_daily_panel['trans_dt'] <= test_end)\n",
    "]\n",
    "oot_panel = per_card_daily_panel[per_card_daily_panel['trans_dt'] > test_end]\n",
    "\n",
    "# Check per-card coverage:\n",
    "train_cards = train_panel['cc_num'].nunique()\n",
    "test_cards = test_panel['cc_num'].nunique()\n",
    "oot_cards = oot_panel['cc_num'].nunique()\n",
    "\n",
    "print(\"Transaction-based splits:\")\n",
    "print(\"Train period:\", train_panel['trans_dt'].min(), \"to\", train_panel['trans_dt'].max())\n",
    "print(\"Test period:\", test_panel['trans_dt'].min(), \"to\", test_panel['trans_dt'].max())\n",
    "print(\"OOT period:\", oot_panel['trans_dt'].min(), \"to\", oot_panel['trans_dt'].max())\n",
    "\n",
    "print(\"\\nNumber of unique cards per period:\")\n",
    "print(\"Train cards:\", train_cards)\n",
    "print(\"Test cards:\", test_cards)\n",
    "print(\"OOT cards:\", oot_cards)\n",
    "\n",
    "print(\"\\nSizes (number of rows):\")\n",
    "print(\"Train rows:\", len(train_panel))\n",
    "print(\"Test rows:\", len(test_panel))\n",
    "print(\"OOT rows:\", len(oot_panel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630936e",
   "metadata": {},
   "source": [
    "## Target Encoding using the Per Card Summary Data Frame based on AMT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca29616",
   "metadata": {},
   "source": [
    "Why use amt (spend amount) instead of transaction count (txn_count)?\n",
    "You have two possible ways to define segments:\n",
    "\n",
    "**Option A**: Transaction Count (txn_count)<br>\n",
    "**Pros**: Measures frequency of card use.<br>\n",
    "**Cons**: Ignores the monetary value of transactions.<br>\n",
    "A card with many small transactions (e.g., coffee daily) may be considered \"heavy,\" while a card with fewer high-value transactions (e.g., luxury purchases) would appear \"light.\"\n",
    "\n",
    "\n",
    "\n",
    "**Option B**: Total Amount (amt) <br>\n",
    "**Pros**: Directly measures actual financial impact, more closely aligned to spending behaviors and profitability.<br>\n",
    "**Cons**: Ignores transaction frequency—high spenders might spend infrequently but substantially.<br>\n",
    "\n",
    "\n",
    "\n",
    "⚖️ Choosing between the two:\n",
    "For predicting revenue or spending behaviors, total amount (amt) is typically preferred because it's directly correlated to business outcomes (profits, revenue, etc.).\n",
    "\n",
    "Transaction frequency is useful if your business goal is engagement or activity levels (frequency of use).\n",
    "\n",
    "Given the goal—\"predicting future credit card spending based on historical behaviors\"—segmenting based on amount spent (amt) is usually more practical and meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac428b00",
   "metadata": {},
   "source": [
    "How does this prevent data leakage?\n",
    "Segments are created solely using historical training-period data.\n",
    "\n",
    "After establishing segments historically, you then apply these stable segments to the test and OOT periods.\n",
    "\n",
    "No future data (test or OOT) influences the segment definitions.\n",
    "\n",
    "This ensures segments are stable, interpretable, and leakage-free.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71b978ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_totals = train_panel.groupby('cc_num')['amt'].sum()\n",
    "quantiles = train_totals.quantile([0.33, 0.66])\n",
    "\n",
    "def spend_segment(amount):\n",
    "    if amount <= quantiles.iloc[0]:\n",
    "        return 'Light'\n",
    "    elif amount <= quantiles.iloc[1]:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "card_segments = train_totals.apply(spend_segment).rename('spender_segment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12dc5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge segments into panel datasets\n",
    "train_panel = train_panel.merge(card_segments, on='cc_num', how='left')\n",
    "test_panel = test_panel.merge(card_segments, on='cc_num', how='left')\n",
    "oot_panel = oot_panel.merge(card_segments, on='cc_num', how='left')\n",
    "\n",
    "# Numeric encoding\n",
    "segment_encoding = {'Light':0, 'Medium':1, 'High':2}\n",
    "train_panel['seg_code'] = train_panel['spender_segment'].map(segment_encoding)\n",
    "test_panel['seg_code'] = test_panel['spender_segment'].map(segment_encoding)\n",
    "oot_panel['seg_code'] = oot_panel['spender_segment'].map(segment_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d4bffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spender_segment\n",
      "High      129878\n",
      "Medium    125469\n",
      "Light     104055\n",
      "Name: count, dtype: int64\n",
      "seg_code\n",
      "2    129878\n",
      "1    125469\n",
      "0    104055\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count segment labels in training panel\n",
    "print(train_panel['spender_segment'].value_counts())\n",
    "print(train_panel['seg_code'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3766565a",
   "metadata": {},
   "source": [
    "Means:\n",
    "\n",
    "You had ~129k transactions in train from high spenders\n",
    "\n",
    "~125k from medium\n",
    "\n",
    "~104k from light\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab0704b",
   "metadata": {},
   "source": [
    "### 📌 Why Segmenting Is Useful for Modeling\n",
    "\n",
    "| Purpose               | Benefit                                                                 |\n",
    "|-----------------------|-------------------------------------------------------------------------|\n",
    "| ✅ Model personalization | Train different models per segment (e.g., heavy spenders may behave differently) |\n",
    "| ✅ Forecasting            | Segment can be an exogenous feature in ARIMA, Prophet, or LSTM          |\n",
    "| ✅ Feature                | You can use `seg_code` as a categorical input in tree models or deep nets |\n",
    "| ✅ Downstream analytics   | Track how each group behaves differently over time                      |\n",
    "| ✅ Targeted strategy      | You might recommend different products or marketing to each group        |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Bonus: Tips for Using the Encoded Segment\n",
    "\n",
    "#### 📈 Plot time series of average spend per segment:\n",
    "```python\n",
    "avg_spend = train_panel.groupby(['trans_dt', 'seg_code'])['amt'].mean().unstack()\n",
    "avg_spend.plot(title=\"Avg Daily Spend by Segment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c3c2b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ❓ Do You Need to Add Segment Encodings to the Transaction Feature Store?\n",
    "\n",
    "**Short answer**: ✅ **Yes, if** you're planning to train models using the `transaction_features` dataset.\n",
    "\n",
    "### Here's Why:\n",
    "- The `seg_code` gives your model access to historical **spending profile** context per transaction.\n",
    "- Many of your predictive tasks (e.g., forecasting, fraud detection, behavior modeling) could benefit from **user segmentation** as an additional feature.\n",
    "\n",
    "### How to Add It:\n",
    "Since `transaction_features` has a `cc_num` column:\n",
    "```python\n",
    "# Merge the segment encodings into the full transaction feature dataset\n",
    "transaction_features = transaction_features.merge(card_segments, on='cc_num', how='left')\n",
    "\n",
    "# Add numeric encoding\n",
    "transaction_features['seg_code'] = transaction_features['spender_segment'].map(segment_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ed853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
